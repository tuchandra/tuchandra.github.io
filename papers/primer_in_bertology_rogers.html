<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <title>
    [Paper] A Primer in BERTology: What We Know About How BERT Works - Tushar Chandra
    </title>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport"
    content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="renderer" content="webkit">
  <meta http-equiv="Cache-Control" content="no-transform" />
  <meta http-equiv="Cache-Control" content="no-siteapp" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="format-detection" content="telephone=no,email=no,adress=no">
  
  <meta name="theme-color" content="#000000" />
  <meta http-equiv="window-target" content="_top" /><meta name="description" content="BERT is one of many large language models taking NLP by storm. But little is known about how or why it works, leading to papers studying it specifically: &amp;ldquo;BERTology.&amp;rdquo; This survey paper synthesizes current research on how BERT works and what remains unknown.
" />
  <meta name="generator" content="Hugo 0.70.0" />
  <title>[Paper] A Primer in BERTology: What We Know About How BERT Works - Tushar Chandra</title>

  
  
  <link rel="stylesheet" href="/css/styles.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.9.0/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/9.15.10/styles/github.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.css">

  <meta property="og:title" content="[Paper] A Primer in BERTology: What We Know About How BERT Works" />
<meta property="og:description" content="BERT is one of many large language models taking NLP by storm. But little is known about how or why it works, leading to papers studying it specifically: &ldquo;BERTology.&rdquo; This survey paper synthesizes current research on how BERT works and what remains unknown." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/papers/primer_in_bertology_rogers.html" />
<meta property="article:published_time" content="2021-03-25T00:00:00+00:00" />
<meta property="article:modified_time" content="2021-03-25T00:00:00+00:00" />
<meta itemprop="name" content="[Paper] A Primer in BERTology: What We Know About How BERT Works">
<meta itemprop="description" content="BERT is one of many large language models taking NLP by storm. But little is known about how or why it works, leading to papers studying it specifically: &ldquo;BERTology.&rdquo; This survey paper synthesizes current research on how BERT works and what remains unknown.">
<meta itemprop="datePublished" content="2021-03-25T00:00:00&#43;00:00" />
<meta itemprop="dateModified" content="2021-03-25T00:00:00&#43;00:00" />
<meta itemprop="wordCount" content="798">



<meta itemprop="keywords" content="reading club," /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="[Paper] A Primer in BERTology: What We Know About How BERT Works"/>
<meta name="twitter:description" content="BERT is one of many large language models taking NLP by storm. But little is known about how or why it works, leading to papers studying it specifically: &ldquo;BERTology.&rdquo; This survey paper synthesizes current research on how BERT works and what remains unknown."/>
</head>
</head>



<body class="h-full flex flex-col justify-between" itemscope itemtype="http://schema.org/WebPage"><header class="bg-green-500" itemscope itemtype="http://schema.org/WPHeader">
  <div class="flex max-w-8xl container mx-auto">
    <div class="hidden sm:inline flex items-center">
      <a href="/">
        <img class="rounded-full m-1 md:m-4" src="/headshot.jpg" width="100" height="100">
      </a>
    </div>
    <div class="w-full px-4 flex flex-col lg:flex-row justify-start md:pt-4 lg:items-center my-4">
      <div class="lg:flex-grow">
        <a href="/">
          <span class="text-lg sm:text-2xl md:text-3xl font-semibold">Tushar Chandra</span>
          <span class="hidden pl-4 lg:inline"><br></span>
          <span class="text-sm md:text-xl font-thin pl-4 md:pl-8 lg:pl-0 lg:text-xl">Data Scientist / Chicago, IL
          </span>
        </a>
      </div>
      <nav class="" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation">
        <ul class="flex lg:justify-between">
          <li class="flex-grow-0 inline pr-1 md:pr-2 xl:px-4">
            <p class="text-center">
              <a href="/about">
                <i class="fas text-green-800 pl-1 md:px-1 text-sm md:text-xl lg:text-2xl fa-mug-hot"></i>
                <span class="font-thin text-sm md:text-lg lg:text-2xl">About</span>
              </a>
            </p>
          </li>
          <li class="flex-grow-0 inline pr-1 md:pr-2 xl:px-4">
            <p class="text-center">
              <a href="/resume">
                <i class="fas text-green-800 pl-1 md:px-1 text-sm md:text-xl lg:text-2xl fa-file"></i>
                <span class="font-thin text-sm md:text-lg lg:text-2xl">Resume</span>
              </a>
            </p>
          </li>
          <li class="flex-grow-0 inline pr-1 md:pr-2 xl:px-4">
            <p class="text-center">
              <a href="/reading">
                <i class="fas text-green-800 pl-1 md:px-1 text-sm md:text-xl lg:text-2xl fa-book-open"></i>
                <span class="font-thin text-sm md:text-lg lg:text-2xl">Reading</span>
              </a>
            </p>
          </li>
          <li class="flex-grow-0 inline pr-1 md:pr-2 xl:px-4">
            <p class="text-center">
              <a href="/categories.html">
                <i class="fas text-green-800 pl-1 md:px-1 text-sm md:text-xl lg:text-2xl fa-folder-open"></i>
                <span class="font-thin text-sm md:text-lg lg:text-2xl">Categories</span>
              </a>
            </p>
          </li>
          <li class="flex-grow-0 inline pr-1 md:pr-2 xl:px-4">
            <p class="text-center">
              <a href="/posts.html">
                <i class="fas text-green-800 pl-1 md:px-1 text-sm md:text-xl lg:text-2xl fa-archive"></i>
                <span class="font-thin text-sm md:text-lg lg:text-2xl">Archives</span>
              </a>
            </p>
          </li>

        </ul>

      </nav>
    </div>

  </div>
</header>
  <div class="container mx-auto max-w-8xl px-4 flex flex-row flex-grow">
    <div class="w-full md:w-3/4">
      
<main class="main" role="main"><div class="content container mx-auto max-w-6xl">
  <article id="-" class="" itemscope itemtype="http://schema.org/BlogPosting">
    <div class="pt-4">
      <span class="font-semibold text-3xl"><h1 itemprop="name">
  <a class="" href="/papers/primer_in_bertology_rogers.html">[Paper] A Primer in BERTology: What We Know About How BERT Works</a>
</h1>
      </span>
      <div class="pb-4"><i class="fas fa-calendar-check text-gray-500 pr-1"></i>
<time datetime="2021-03-25 00:00:00 &#43;0000 UTC" class="text-sm text-gray-600"
  itemprop="datePublished">2021-03-25</time>
<span class="text-sm text-gray-600" itemprop="wordCount">
	<i class="fas fa-pencil-alt text-gray-500 pl-4 pr-2"></i>798 words
</span><i class="fas fa-folder-open text-gray-500 pl-4 pr-1"></i>
<a class="text-sm text-gray-600" href=" /categories/papers.html"> papers, </a>
<span class="article-tag">
  <i class="fas fa-tag text-gray-500 pl-4 pr-1"></i>
  <a class="text-sm text-gray-600" href="/tags/reading-club.html"> reading club, </a>
</span><span class="article-category text-sm text-gray-600 pl-4">
  <i class="fa fa-users "></i>
  Anna Rogers, Olga Kovaleva, Anna Rumshisky
</span>
      </div>
    </div>
    <div class="rich-text" itemprop="articleBody">
      <p>BERT is one of many large language models taking NLP by storm. But little is known about how or why it works, leading to papers studying it specifically: &ldquo;BERTology.&rdquo; This survey paper synthesizes current research on how BERT works and what remains unknown.</p>
<p><strong>Authors</strong>: Anna Rogers, Olga Kovaleva, Anna Rumshisky</p>
<p><strong>Link</strong>: <a href="https://arxiv.org/abs/2002.12327">arXiv</a></p>
<h2 id="background-and-motivation"><a class="not-rich" href="#background-and-motivation"><i class="fas fa-link"></i></a> Background and motivation</h2>
<p>BERT is the most well-known language model based on Transformers (aka &ldquo;attention,&rdquo; which was <em>not</em> clear to me early on). It works well on a variety of benchmarks, but like many language models, it&rsquo;s not always clear why.</p>
<p>It&rsquo;s also unclear what&rsquo;s happening under the hood; what is the nature of BERT&rsquo;s &ldquo;understanding&rdquo; of language? What kind of linguistic knowledge does it have?</p>
<p>These kinds of questions motivate what the authors call &ldquo;BERTology&rdquo; (literally &ldquo;the study of BERT&rdquo;). This work is a survey paper of 150 studies (!) attempting to understand BERT.</p>
<h2 id="what-does-bert-know"><a class="not-rich" href="#what-does-bert-know"><i class="fas fa-link"></i></a> What does BERT know?</h2>
<p>The authors gather results from dozens of other papers studying BERT. Some of the highlights include:</p>
<ul>
<li>BERT encodes some information about parts of speech and syntax</li>
<li>BERT does not directly encode syntax structure, but this can be recovered from token embeddings</li>
<li>BERT does not understand negation, and its predictions are invariant to some kinds of malformed inputs</li>
<li>BERT is &ldquo;brittle to named entity replacements,&rdquo; suggesting that it doesn&rsquo;t genericize named entities</li>
<li>BERT struggles with real-world inference and cannot reason based off its knowledge</li>
</ul>
<p>What about more specific linguistic knowledge? How is that stored? In embeddings, and <em>contextualized</em> embeddings specifically. (Two embeddings for identical words will differe slightly by context, and they&rsquo;ll differ more if you take the embeddings from later BERT layers.) The embedding can also differ based on the position of the word in the sentence (the authors call this undesirable - why?).</p>
<p>Lower layers of BERT have the most information about word order; middle layers have the most syntactic information; and the final layers are the most task-specific. Semantics are apparently spread across the model.</p>
<p>There&rsquo;s an entire section on training BERT, too. These papers studied the impact of things like:</p>
<ul>
<li>model architecture (number of attention heads, number of layers)</li>
<li>large-batch training</li>
<li>masked language models (?), including how, what, and where to mask</li>
<li>adding explicit linguistic information or other structured knowledge</li>
<li>pre-training, fine-tuning, and different ways of doing each</li>
</ul>
<h2 id="what-comes-next"><a class="not-rich" href="#what-comes-next"><i class="fas fa-link"></i></a> What comes next?</h2>
<p>The last section of the paper talks about directions for future research. Leaving aside the fact that there are several research directions for studying a single language model, they suggest:</p>
<ul>
<li>developing benchmarks that require verbal reasoning (where BERT can&rsquo;t just use shallow heuristics, which it&rsquo;s bound to learn in any dataset large enough)</li>
<li>developing benchmarks for more linguistic competence</li>
<li>studying how to teach reasoning (BERT has lots of facts, but can&rsquo;t really combine them)</li>
<li>understanding what happens at inference (what knowledge gets used? what features are important?)</li>
</ul>
<h2 id="my-thoughts"><a class="not-rich" href="#my-thoughts"><i class="fas fa-link"></i></a> My thoughts</h2>
<p>It&rsquo;s kind of amazing that NLP research is in a place where we can have an <em>entire survey paper</em>—not just individual papers—on how a <em>single</em> model works. The authors said that they covered over 150 studies on BERT alone! I&rsquo;m sure that number has grown since publication, too.</p>
<p>I think this speaks to how centralized the field is around a few very large language models. It gives more importance to the concerns raised in the <a href="/papers/stochastic_parrots_bender.html">Stochastic Parrots paper</a>. We understand so little about these models, and it&rsquo;s clear that they lack any kind of reasoning ability!</p>
<p>This paper shows that there are still many open questions about how BERT works. There are questions about other large language models, too. We can add questions about how BERT is used, too. (I&rsquo;m sure a 2021 version of <a href="https://nyupress.org/9781479837243/algorithms-of-oppression/">Algorithms of Oppression</a> would have plenty to say!)</p>
<p>Survey papers are challenging for me to read, though. I appreciate the depth of the literature review; this is clearly very thorough. But I wish the authors had done more to synthesize the information shown and make it a little more cohesive. It felt more like reading a list rather than an article.</p>
<p>More generally (and this isn&rsquo;t a criticism of the paper), this kind of paper makes me pessimistic about the future of NLP research. It feels like there are large language models that <em>happen</em> to work well on the specific things we evaluate them on. But we don&rsquo;t have a general purpose understanding of what they&rsquo;re actually learning, what different modifications will do, or why they work the way that they do.</p>
<p>It seems like it&rsquo;s impossible to reason about the models that are taking over the field. What do we do when our tools are so opaque that, after hundreds of papers, we still don&rsquo;t understand their capabilities or why they work as well as they do?</p>
<p>I don&rsquo;t really know, but I&rsquo;m glad the authors are asking.</p>

      <p><i class="fas fa-square font-green-800"></i></p>
    </div>
    <div class="font-semibold text-green-800 pt-4">
      <a href="/"><i class="fas fa-arrow-left"></i> Return home</a>
    </div>
  </article>
</div><nav class="bar bar-footer clearfix" data-stick-bottom>
    <div class="bar-inner">
        <ul class="pager pull-left">
            <li class="prev">
                <a href="/papers/formalizing_trust_ai_goldberg.html" title="[Paper] Formalizing Trust in Artificial Intelligence: Prerequisites, Causes and Goals of Human Trust in AI"><i
                        class="icon icon-angle-left"
                        aria-hidden="true"></i><span>&nbsp;&nbsp;</span></a>
            </li>
            <li class="next">
                <a href="/posts/starting_tempus.html"
                    title="Personal news: starting a new job!"><span>&nbsp;&nbsp;</span><i
                        class="icon icon-angle-right" aria-hidden="true"></i></a>
            </li>
        </ul>
    </div>
</nav>

</main>
    </div>
    <div class="hidden w-0 md:block md:w-1/4 md:pt-8 md:pl-12"><aside class="" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="pb-8">
    <h3 class="font-semibold text-xl">Categories</h3>
    <ul class="">
        <li class="">+ <a href="/categories/books.html" class="font-bold text-green-900">books
            </a><span class="">25</span>
        </li>
        <li class="">+ <a href="/categories/general.html" class="font-bold text-green-900">general
            </a><span class="">45</span>
        </li>
        <li class="">+ <a href="/categories/papers.html" class="font-bold text-green-900">papers
            </a><span class="">96</span>
        </li>
        <li class="">+ <a href="/categories/projects.html" class="font-bold text-green-900">projects
            </a><span class="">7</span>
        </li>
        <li class="">+ <a href="/categories/self.html" class="font-bold text-green-900">self
            </a><span class="">5</span>
        </li>
        <li class="">+ <a href="/categories/spark.html" class="font-bold text-green-900">spark
            </a><span class="">34</span>
        </li>
        <li class="">+ <a href="/categories/what-i-read.html" class="font-bold text-green-900">what-i-read
            </a><span class="">53</span>
        </li>
    </ul>
</div>
<div class="pb-4">
    <h3 class="font-semibold text-xl">Recent Posts</h3>
    <ul class="recent-post-list list-unstyled no-thumbnail">
        <li class="pb-4">
            <p>
                <a href="/papers/data_cascades_ai_sambasivan.html" class="font-bold text-green-900">[Paper] &#39;Everyone wants to do the model work, not the data work&#39;: Data Cascades in High-Stakes AI</a>
                <time datetime="2021-04-07 00:00:00 &#43;0000 UTC" itemprop="datePublished" class="text-gray-600 text-sm">2021-04-07
                </time>
            </p>

        </li>
        <li class="pb-4">
            <p>
                <a href="/papers/suicidal_on_sunday_pendse.html" class="font-bold text-green-900">[Paper] &#39;Can I Not Be Suicidal on a Sunday?&#39; Understanding Technology-Mediated Pathways to Mental Health Support</a>
                <time datetime="2021-04-04 00:00:00 &#43;0000 UTC" itemprop="datePublished" class="text-gray-600 text-sm">2021-04-04
                </time>
            </p>

        </li>
        <li class="pb-4">
            <p>
                <a href="/posts/starting_tempus.html" class="font-bold text-green-900">Personal news: starting a new job!</a>
                <time datetime="2021-03-31 00:00:00 &#43;0000 UTC" itemprop="datePublished" class="text-gray-600 text-sm">2021-03-31
                </time>
            </p>

        </li>
        <li class="pb-4">
            <p>
                <a href="/papers/primer_in_bertology_rogers.html" class="font-bold text-green-900">[Paper] A Primer in BERTology: What We Know About How BERT Works</a>
                <time datetime="2021-03-25 00:00:00 &#43;0000 UTC" itemprop="datePublished" class="text-gray-600 text-sm">2021-03-25
                </time>
            </p>

        </li>
        <li class="pb-4">
            <p>
                <a href="/papers/formalizing_trust_ai_goldberg.html" class="font-bold text-green-900">[Paper] Formalizing Trust in Artificial Intelligence: Prerequisites, Causes and Goals of Human Trust in AI</a>
                <time datetime="2021-03-11 00:00:00 &#43;0000 UTC" itemprop="datePublished" class="text-gray-600 text-sm">2021-03-11
                </time>
            </p>

        </li>
    </ul>
</div><div class="pb-12">
    <h3 class="font-semibold text-xl">Tags</h3>
    <ul class="">
        <li class="">+ <a href="/tags/#republic.html" class="font-bold text-green-900">#republic
            </a><span class="">5</span>
        </li>
        <li class="">+ <a href="/tags/chi.html" class="font-bold text-green-900">chi
            </a><span class="">1</span>
        </li>
        <li class="">+ <a href="/tags/chi2019.html" class="font-bold text-green-900">chi2019
            </a><span class="">3</span>
        </li>
        <li class="">+ <a href="/tags/chi2020.html" class="font-bold text-green-900">chi2020
            </a><span class="">20</span>
        </li>
        <li class="">+ <a href="/tags/chi2021.html" class="font-bold text-green-900">chi2021
            </a><span class="">3</span>
        </li>
        <li class="">+ <a href="/tags/cscw2020.html" class="font-bold text-green-900">cscw2020
            </a><span class="">8</span>
        </li>
        <li class="">+ <a href="/tags/facct2021.html" class="font-bold text-green-900">facct2021
            </a><span class="">3</span>
        </li>
        <li class="">+ <a href="/tags/fat2020.html" class="font-bold text-green-900">fat2020
            </a><span class="">2</span>
        </li>
        <li class="">+ <a href="/tags/iclr.html" class="font-bold text-green-900">iclr
            </a><span class="">2</span>
        </li>
        <li class="">+ <a href="/tags/icwsm2020.html" class="font-bold text-green-900">icwsm2020
            </a><span class="">5</span>
        </li>
        <li class="">+ <a href="/tags/indistractable.html" class="font-bold text-green-900">indistractable
            </a><span class="">2</span>
        </li>
        <li class="">+ <a href="/tags/neurips.html" class="font-bold text-green-900">neurips
            </a><span class="">2</span>
        </li>
        <li class="">+ <a href="/tags/password-tool.html" class="font-bold text-green-900">password-tool
            </a><span class="">6</span>
        </li>
        <li class="">+ <a href="/tags/reading-club.html" class="font-bold text-green-900">reading-club
            </a><span class="">28</span>
        </li>
        <li class="">+ <a href="/tags/recsys.html" class="font-bold text-green-900">recsys
            </a><span class="">1</span>
        </li>
        <li class="">+ <a href="/tags/site.html" class="font-bold text-green-900">site
            </a><span class="">6</span>
        </li>
        <li class="">+ <a href="/tags/www.html" class="font-bold text-green-900">www
            </a><span class="">3</span>
        </li>
    </ul>
</div>
</aside><footer class="" itemscope itemtype="http://schema.org/WPFooter">
  <div class="py-4 text-sm">
    &copy; 2019 - 2020 Tushar Chandra
    <p style="margin-bottom: 4px">All opinions are my own.</p>
    <div class="publishby">Powered by Hugo with a custom theme.</div>
  </div>
  <ul class="">
    <li class="inline text-xl pr-4">
      <a href="https://github.com/tuchandra" target="_blank" title="github">
        <i class="fab fa-github"></i>
      </a>
    </li>
    <li class="inline text-xl pr-4">
      <a href="https://www.linkedin.com/in/tushar-chandra-76a623b6/" target="_blank" title="linkedin">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
    <li class="inline text-xl pr-4">
      <a href="mailto:me@tusharc.dev" target="_blank" title="envelope">
        <i class="fas fa-envelope"></i>
      </a>
    </li>
    <li class="inline text-xl pr-4">
      <a href="/index.xml" target="_blank" title="rss">
        <i class="fas fa-rss"></i>
      </a>
    </li>

  </ul>
</footer>
    </div>
  </div>
  <div class="mx-auto mt-auto w-full px-4 bg-green-500 mt-4 md:py-8">
    <div class="md:invisible md:hidden max-w-8xl"><footer class="" itemscope itemtype="http://schema.org/WPFooter">
  <div class="py-4 text-sm">
    &copy; 2019 - 2020 Tushar Chandra
    <p style="margin-bottom: 4px">All opinions are my own.</p>
    <div class="publishby">Powered by Hugo with a custom theme.</div>
  </div>
  <ul class="">
    <li class="inline text-xl pr-4">
      <a href="https://github.com/tuchandra" target="_blank" title="github">
        <i class="fab fa-github"></i>
      </a>
    </li>
    <li class="inline text-xl pr-4">
      <a href="https://www.linkedin.com/in/tushar-chandra-76a623b6/" target="_blank" title="linkedin">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
    <li class="inline text-xl pr-4">
      <a href="mailto:me@tusharc.dev" target="_blank" title="envelope">
        <i class="fas fa-envelope"></i>
      </a>
    </li>
    <li class="inline text-xl pr-4">
      <a href="/index.xml" target="_blank" title="rss">
        <i class="fas fa-rss"></i>
      </a>
    </li>

  </ul>
</footer>
    </div>
  </div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_SVG"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
            showMathMenu: false, //disables context menu
            tex2jax: {
            inlineMath: [ ['$','$'], ['\\(','\\)'] ]
           }
    });
</script>







<script data-goatcounter="https://tusharc.goatcounter.com/count" async src="//gc.zgo.at/count.js">
</script>
</body>

</html>