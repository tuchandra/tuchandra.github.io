<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <title>
        [Paper] Proposed Guidelines for the Responsible Use of Explainable Machine Learning - Tushar Chandra
      </title>
    <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport"
    content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="renderer" content="webkit">
  <meta http-equiv="Cache-Control" content="no-transform" />
  <meta http-equiv="Cache-Control" content="no-siteapp" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="format-detection" content="telephone=no,email=no,adress=no">
  
  <meta name="theme-color" content="#000000" />
  <meta http-equiv="window-target" content="_top" />
  
  <meta name="description" content="This is a paper attempting to make explainable AI (XAI) a little more concrete by developing proposed guidelines for its use. This was a workshop paper at NeurIPS 2019, then a talk at the KDD 2019 workshop.
" />
  <meta name="generator" content="Hugo 0.62.2 with theme pure" />
  <title>[Paper] Proposed Guidelines for the Responsible Use of Explainable Machine Learning - Tushar Chandra</title>
  

  <link rel="stylesheet" href="/css/style.css">
  <link rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/9.15.10/styles/github.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.css">
  <meta property="og:title" content="[Paper] Proposed Guidelines for the Responsible Use of Explainable Machine Learning" />
<meta property="og:description" content="This is a paper attempting to make explainable AI (XAI) a little more concrete by developing proposed guidelines for its use. This was a workshop paper at NeurIPS 2019, then a talk at the KDD 2019 workshop." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/papers/proposed_guidelines_xai_hall.html" />
<meta property="article:published_time" content="2020-02-17T00:00:00-06:00" />
<meta property="article:modified_time" content="2020-02-17T00:00:00-06:00" />
<meta itemprop="name" content="[Paper] Proposed Guidelines for the Responsible Use of Explainable Machine Learning">
<meta itemprop="description" content="This is a paper attempting to make explainable AI (XAI) a little more concrete by developing proposed guidelines for its use. This was a workshop paper at NeurIPS 2019, then a talk at the KDD 2019 workshop.">
<meta itemprop="datePublished" content="2020-02-17T00:00:00-06:00" />
<meta itemprop="dateModified" content="2020-02-17T00:00:00-06:00" />
<meta itemprop="wordCount" content="911">



<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="[Paper] Proposed Guidelines for the Responsible Use of Explainable Machine Learning"/>
<meta name="twitter:description" content="This is a paper attempting to make explainable AI (XAI) a little more concrete by developing proposed guidelines for its use. This was a workshop paper at NeurIPS 2019, then a talk at the KDD 2019 workshop."/>
</head>
  </head>
  

  <body class="main-center theme-blue" itemscope itemtype="http://schema.org/WebPage"><header class="header" itemscope itemtype="http://schema.org/WPHeader">
    <div class="slimContent">
      <div class="navbar-header">
        <div class="profile-block text-center">
          <a id="avatar" href="/">
            <img class="img-circle" src="/headshot.jpg" width="200" height="200">
          </a>
          <h2 id="name" class="hidden-xs hidden-sm"><b>Tushar Chandra</b></h2>
          <h3 id="title" class="hidden-xs hidden-sm hidden-md">Data Scientist</h3>
          <small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> Chicago, IL</small>
        </div>

        
        <nav id="mobile-navbar" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation">
          <ul class="mobile-nav">
              <a href="/">
                <li class="menu-item-home">
                    <i class="icon icon-home-fill"></i>
                  <span class="menu-title"><br>Home</span>
                </li>
              </a>
              <a href="/about">
                <li class="menu-item-about">
                    <i class="icon icon-cup-fill"></i>
                  <span class="menu-title"><br>About</span>
                </li>
              </a>
              <a href="/resume">
                <li class="menu-item-resume">
                    <i class="icon icon-file"></i>
                  <span class="menu-title"><br>Resume</span>
                </li>
              </a>
              <a href="/reading">
                <li class="menu-item-reading">
                    <i class="icon icon-shu-fill"></i>
                  <span class="menu-title"><br>Reading</span>
                </li>
              </a>
              <a href="/categories.html">
                <li class="menu-item-categories">
                    <i class="icon icon-folder-open"></i>
                  <span class="menu-title"><br>Categories</span>
                </li>
              </a>
              <a href="/posts.html">
                <li class="menu-item-archives">
                    <i class="icon icon-archives-fill"></i>
                  <span class="menu-title"><br>Archives</span>
                </li>
              </a>

          </ul>
        </nav>
      </div>
      <nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation">
        <ul class="nav navbar-nav main-nav">
            <li class="menu-item menu-item-home">
                <a href="/">
                    <i class="icon icon-home-fill"></i>
                  <span class="menu-title">Home</span>
                </a>
            </li>
            <li class="menu-item menu-item-about">
                <a href="/about">
                    <i class="icon icon-cup-fill"></i>
                  <span class="menu-title">About</span>
                </a>
            </li>
            <li class="menu-item menu-item-resume">
                <a href="/resume">
                    <i class="icon icon-file"></i>
                  <span class="menu-title">Resume</span>
                </a>
            </li>
            <li class="menu-item menu-item-reading">
                <a href="/reading">
                    <i class="icon icon-shu-fill"></i>
                  <span class="menu-title">Reading</span>
                </a>
            </li>
            <li class="menu-item menu-item-categories">
                <a href="/categories.html">
                    <i class="icon icon-folder-open"></i>
                  <span class="menu-title">Categories</span>
                </a>
            </li>
            <li class="menu-item menu-item-archives">
                <a href="/posts.html">
                    <i class="icon icon-archives-fill"></i>
                  <span class="menu-title">Archives</span>
                </a>
            </li>

        </ul>
      </nav>
    </div>
  </header>
<aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    <div class="widget">
    <h3 class="widget-title"> Categories</h3>
    <div class="widget-body">
        <ul class="category-list">
            <li class="category-list-item"><a href="/categories/books.html" class="category-list-link">books</a><span class="category-list-count">5</span></li>
            <li class="category-list-item"><a href="/categories/chi2020.html" class="category-list-link">chi2020</a><span class="category-list-count">4</span></li>
            <li class="category-list-item"><a href="/categories/general.html" class="category-list-link">general</a><span class="category-list-count">29</span></li>
            <li class="category-list-item"><a href="/categories/papers.html" class="category-list-link">papers</a><span class="category-list-count">29</span></li>
            <li class="category-list-item"><a href="/categories/projects.html" class="category-list-link">projects</a><span class="category-list-count">3</span></li>
            <li class="category-list-item"><a href="/categories/self.html" class="category-list-link">self</a><span class="category-list-count">3</span></li>
            <li class="category-list-item"><a href="/categories/spark.html" class="category-list-link">spark</a><span class="category-list-count">13</span></li>
            <li class="category-list-item"><a href="/categories/what-i-read.html" class="category-list-link">what-i-read</a><span class="category-list-count">23</span></li>
        </ul>
    </div>
</div>
<div class="widget">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget-body">
        <ul class="recent-post-list list-unstyled no-thumbnail">
            <li>
                <div class="item-inner">
                    <p class="item-title">
                        <a href="/papers/finsta_authentic_disagreebale_taber.html" class="title">[Paper] &#39;On Finsta I can say Hail Satan&#39;: Being Authentic but Disagreeable on Instagram</a>
                    </p>
                    <p class="item-date">
                        <time datetime="2020-04-28 00:00:00 -0500 CDT" itemprop="datePublished">2020-04-28</time>
                    </p>
                </div>
            </li>
            <li>
                <div class="item-inner">
                    <p class="item-title">
                        <a href="/papers/chi_2020_nordic_talks.html" class="title">A variety of talks from CHI 2020</a>
                    </p>
                    <p class="item-date">
                        <time datetime="2020-04-27 00:00:00 -0500 CDT" itemprop="datePublished">2020-04-27</time>
                    </p>
                </div>
            </li>
            <li>
                <div class="item-inner">
                    <p class="item-title">
                        <a href="/posts/password_tool_2.html" class="title">Attempts at cracking a rescue password</a>
                    </p>
                    <p class="item-date">
                        <time datetime="2020-04-27 00:00:00 &#43;0000 UTC" itemprop="datePublished">2020-04-27</time>
                    </p>
                </div>
            </li>
            <li>
                <div class="item-inner">
                    <p class="item-title">
                        <a href="/papers/data_literacy_sports_clegg.html" class="title">[Paper] Data Everyday: Data Literacy Practices in a Division I Sports Context</a>
                    </p>
                    <p class="item-date">
                        <time datetime="2020-04-26 00:00:00 -0500 CDT" itemprop="datePublished">2020-04-26</time>
                    </p>
                </div>
            </li>
            <li>
                <div class="item-inner">
                    <p class="item-title">
                        <a href="/posts/password_tool_1.html" class="title">My newest project: a Pokemon Mystery Dungeon password tool</a>
                    </p>
                    <p class="item-date">
                        <time datetime="2020-04-26 00:00:00 &#43;0000 UTC" itemprop="datePublished">2020-04-26</time>
                    </p>
                </div>
            </li>
        </ul>
    </div>
</div>
  </div>
</aside>

    
    
<main class="main" role="main"><div class="content">
  <article id="-" class="article article-type-" itemscope
    itemtype="http://schema.org/BlogPosting">
    
    <div class="article-header">
      <h1 itemprop="name">
  <a
    class=""
    href="/papers/proposed_guidelines_xai_hall.html"
    >[Paper] Proposed Guidelines for the Responsible Use of Explainable Machine Learning</a
  >
</h1>

      <div class="article-meta">
        <span class="article-date">
  <i class="icon icon-calendar-check"></i>
<a href="/papers/proposed_guidelines_xai_hall.html">
  <time datetime="2020-02-17 00:00:00 -0600 CST" itemprop="datePublished">2020-02-17</time>
</a>
</span><span class="article-category">
  <i class="icon icon-folder"></i>
    <a class="article-category-link" href="/categories/papers.html"> papers, </a>
</span>

        
      </div>
    </div>
    <div class="article-entry marked-body js-toc-content" itemprop="articleBody">
      <p>This is a paper attempting to make explainable AI (XAI) a little more concrete by developing proposed guidelines for its use. This was a workshop paper at NeurIPS 2019, then a talk at the KDD 2019 workshop.</p>
<p><strong>Authors</strong>: Patrick Hall, Navdeep Gill, Nicholas Schmidt</p>
<p><strong>Link</strong>: on <a href="https://arxiv.org/abs/1906.03533">arXiv</a>, plus related slides <a href="https://github.com/jphall663/kdd_2019">on Github</a></p>
<p><strong>How I found this paper</strong>: I met the lead author at Spark + AI Summit where he gave <a href="https://tusharc.dev/spark/interpretable_ai.html">a talk on ML interpretability</a> (link is to my notes). This paper came onto my radar after following some links from the talk late last year.</p>
<h2 id="summary">Summary</h2>
<p>Explainable AI (XAI) has been in vogue recently, as ML models continue to become more complex and more opaque. It's great for enabling humans to learn from ML and to understand why a model behaved the way it did, but (like any technology) it can be misused.</p>
<h3 id="definitions">Definitions</h3>
<p>The paper includes definitions that will be useful for this blog post and future ones:</p>
<ul>
<li><strong>Interpretability</strong>: the ability to explain or present in understandable terms to a human</li>
<li><strong>Explanation</strong>: collection of visual and/or interactive artifacts that provide a user with sufficient description of a model's behavior to accurately perfrom tasks like evaluation, trusting, predicting, or improving a model</li>
<li><strong>Explainable ML</strong>: mostly (local or global) post-hoc analysis, like feature importances (Shapley values), surrogate models (LIME, surrogate decision trees), or visualizations (partial dependence plots, individual conditional expectations)</li>
<li><strong>Interpretable ML</strong>: white-box models (linear models, decision trees, rule-based models, etc.)</li>
<li><strong>Unwanted sociological bias</strong>: one of many forms of discrimination, including overt discrimination or disparate treatment or impact.</li>
</ul>
<p>A model is <em>biased</em> if (1) group membership is not independent of the likelihood of a favorable outcome, or (2) membership in a subset of a group is not independent of that likelihood. The discrimination that causes this bias may or may not be illegal.</p>
<h3 id="proposed-guidelines">Proposed guidelines</h3>
<p><strong>Use explanations to enable understanding</strong>: explanations typically increase understanding and transparency, but not necessarily trust. It's possible to understand a model without trusting it (e.g., when a feature is overemphasized, so you know <em>why</em> it's making a decision but don't expect it to work when that feature varies). It's possible to trust a model without understanding it (pick your favorite neural network).</p>
<p><strong>Learn how explainable ML is used for nefarious purposes</strong>: these methods can cover for intentionally misused black-boxes or enable stealing of training data. Understanding the dark side of this technology can help to detect abuse.</p>
<p><strong>Augment surrogate models with direct explanation</strong>: &ldquo;models of models&rdquo; (surrogates) can be helpful, but they should be used alongside techniques that use the original model (e.g., Shapley values, partial dependence plots, ALE, or ICE plots).</p>
<p><strong>Use highly interpretable mechanisms for mission- or life-critical ML</strong>: given the fact that explaining black boxes is still super challenging, if a decision is life-altering or high-value, use an interpretable model. Interpretable models are also recommended for when a value needs to be appealable.</p>
<p>Regulatory statutes are likely to require interpretable models (and in some cases, they already do). Adverse action notices (e.g., why you got denied for a credit card) are also required in many cases, and models that enable these are required by default.</p>
<p>A corollary to the last guideline that I like is to use explanations alongside testing for disparate impact. Explanations increase transparency, but testing for disparate impact increases trust. As discussed in the first section, these are not the same!</p>
<h2 id="thoughts-connections-and-questions">Thoughts, connections, and questions</h2>
<p>I like how this paper gives <em>practical</em> advice in the wake of a wave of coverage on explainable and interpretable AI. As ML systems continue to proliferate into everyday decision making, it's important that we continue to develop techniques to understand them.</p>
<p>Development of new techniques in this field appears to be tightly linked to tech policy. It seems to me that interpretable and explainable AI are starting to receive more attention as <a href="https://www.bbc.com/news/technology-51178198">calls to regulate AI</a> increase and as stories of AI bias <a href="/posts/imagenet_bias_wired.html">continue to appear</a> in the news. There is likely to be a hodgepodge of regulations that allow certain things in certain situations, and I think we'll see this field grow quickly as these regulations continue to appear.</p>
<p>I really love Hall's point about understanding and trust being different. I think this will continue to be an important distinction as we (regular people) are on the &ldquo;receiving end&rdquo; of ML models. For example, I trust the neural nets behind my Google Home to process what I say and give me a reasonable response. I don't, however, understand them at all. That's fine for this application, though, because I don't <em>need</em> to understand how it parses my speech.</p>
<p>To that point, though, there appear to be subfields of ML where interpetable models have no place at all; a lot of NLP and computer vision applications seem this way based on my (surface level) understanding. I'm not aware of any interpetable models that can reach close to SOTA on image classification or speech recognition tasks.</p>
<p>Other have warned against the use of black box models in high-stakes decisions; Cynthia Rudin did so in a <a href="https://www.nature.com/articles/s42256-019-0048-x">paper</a> that received some attention. In applications where CV causes an impact on human lives (self-driving cars, anyone?), what are we to do? One possible solution would be to treat the output of the CV algorithm as fact, and have the decision making that follows it be interpretable; but this might be sidestepping the problem (e.g., &ldquo;why didn't the car recognize that as a person?&quot;). This is an interesting problem.</p>
    </div>
    <div class="article-footer">
    </div>
  </article>
</div><nav class="bar bar-footer clearfix" data-stick-bottom>
    <div class="bar-inner">
        <ul class="pager pull-left">
            <li class="prev">
                <a href="/posts/advice_for_reading_papers_ng.html" title="Andrew Ng&#39;s advice for reading research papers"><i
                        class="icon icon-angle-left"
                        aria-hidden="true"></i><span>&nbsp;&nbsp;Older</span></a>
            </li>
            <li class="next">
                <a href="/what_i_read/20200222.html"
                    title="What I read this week (February 16 - February 22)"><span>Newer&nbsp;&nbsp;</span><i
                        class="icon icon-angle-right" aria-hidden="true"></i></a>
            </li>
        </ul>
    </div>
</nav>


</main><footer class="footer" itemscope itemtype="http://schema.org/WPFooter">
<ul class="social-links">
    <li><a href="https://github.com/tuchandra" target="_blank" title="github" data-toggle=tooltip data-placement=top >
            <i class="icon icon-github"></i></a></li>
    <li><a href="https://www.linkedin.com/in/tushar-chandra-76a623b6/" target="_blank" title="linkedin" data-toggle=tooltip data-placement=top >
            <i class="icon icon-linkedin"></i></a></li>
    <li><a href="/index.xml" target="_blank" title="rss" data-toggle=tooltip data-placement=top >
            <i class="icon icon-rss"></i></a></li>
</ul>
  <div class="copyright">
    &copy; 2019 - 2020 Tushar Chandra
    <p style="margin-bottom: 4px">All opinions are my own.</p>
    <div class="publishby">
        Theme: <a href="https://github.com/xiaoheiAh/hugo-theme-pure" target="_blank">Hugo Pure</a>.
    </div>
  </div>
</footer>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_SVG"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
            showMathMenu: false, //disables context menu
            tex2jax: {
            inlineMath: [ ['$','$'], ['\\(','\\)'] ]
           }
    });
</script>


<script type="text/javascript" src="https://cdn.staticfile.org/highlight.js/9.15.10/highlight.min.js"></script>
<script type="text/javascript" src="https://cdn.staticfile.org/highlight.js/9.15.10/languages/python.min.js"></script><script>
    hljs.configure({
        tabReplace: '    ', 
        classPrefix: ''     
        
    })
    hljs.initHighlightingOnLoad();
</script>

  </body>
</html>
