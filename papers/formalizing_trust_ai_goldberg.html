<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <title>
    [Paper] Formalizing Trust in Artificial Intelligence: Prerequisites, Causes and Goals of Human Trust in AI - Tushar Chandra
    </title>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport"
    content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="renderer" content="webkit">
  <meta http-equiv="Cache-Control" content="no-transform" />
  <meta http-equiv="Cache-Control" content="no-siteapp" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="format-detection" content="telephone=no,email=no,adress=no">
  
  <meta name="theme-color" content="#000000" />
  <meta http-equiv="window-target" content="_top" /><meta name="description" content="What does it mean for a human to trust an AI system? What are the requirements for it to be present, and how do we know if it is? This paper, which just appeared at FAccT, explores these questions.
" />
  <meta name="generator" content="Hugo 0.70.0" />
  <title>[Paper] Formalizing Trust in Artificial Intelligence: Prerequisites, Causes and Goals of Human Trust in AI - Tushar Chandra</title>

  
  
  <link rel="stylesheet" href="/css/styles.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.9.0/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/9.15.10/styles/github.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.css">

  <meta property="og:title" content="[Paper] Formalizing Trust in Artificial Intelligence: Prerequisites, Causes and Goals of Human Trust in AI" />
<meta property="og:description" content="What does it mean for a human to trust an AI system? What are the requirements for it to be present, and how do we know if it is? This paper, which just appeared at FAccT, explores these questions." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/papers/formalizing_trust_ai_goldberg.html" />
<meta property="article:published_time" content="2021-03-11T00:00:00+00:00" />
<meta property="article:modified_time" content="2021-03-11T00:00:00+00:00" />
<meta itemprop="name" content="[Paper] Formalizing Trust in Artificial Intelligence: Prerequisites, Causes and Goals of Human Trust in AI">
<meta itemprop="description" content="What does it mean for a human to trust an AI system? What are the requirements for it to be present, and how do we know if it is? This paper, which just appeared at FAccT, explores these questions.">
<meta itemprop="datePublished" content="2021-03-11T00:00:00&#43;00:00" />
<meta itemprop="dateModified" content="2021-03-11T00:00:00&#43;00:00" />
<meta itemprop="wordCount" content="1187">



<meta itemprop="keywords" content="reading club,facct2021," /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="[Paper] Formalizing Trust in Artificial Intelligence: Prerequisites, Causes and Goals of Human Trust in AI"/>
<meta name="twitter:description" content="What does it mean for a human to trust an AI system? What are the requirements for it to be present, and how do we know if it is? This paper, which just appeared at FAccT, explores these questions."/>
</head>
</head>



<body class="h-full flex flex-col justify-between" itemscope itemtype="http://schema.org/WebPage"><header class="bg-green-500" itemscope itemtype="http://schema.org/WPHeader">
  <div class="flex max-w-8xl container mx-auto">
    <div class="hidden sm:inline flex items-center">
      <a href="/">
        <img class="rounded-full m-1 md:m-4" src="/headshot.jpg" width="100" height="100">
      </a>
    </div>
    <div class="w-full px-4 flex flex-col lg:flex-row justify-start md:pt-4 lg:items-center my-4">
      <div class="lg:flex-grow">
        <a href="/">
          <span class="text-lg sm:text-2xl md:text-3xl font-semibold">Tushar Chandra</span>
          <span class="hidden pl-4 lg:inline"><br></span>
          <span class="text-sm md:text-xl font-thin pl-4 md:pl-8 lg:pl-0 lg:text-xl">Data Scientist / Chicago, IL
          </span>
        </a>
      </div>
      <nav class="" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation">
        <ul class="flex lg:justify-between">
          <li class="flex-grow-0 inline pr-1 md:pr-2 xl:px-4">
            <p class="text-center">
              <a href="/about">
                <i class="fas text-green-800 pl-1 md:px-1 text-sm md:text-xl lg:text-2xl fa-mug-hot"></i>
                <span class="font-thin text-sm md:text-lg lg:text-2xl">About</span>
              </a>
            </p>
          </li>
          <li class="flex-grow-0 inline pr-1 md:pr-2 xl:px-4">
            <p class="text-center">
              <a href="/resume">
                <i class="fas text-green-800 pl-1 md:px-1 text-sm md:text-xl lg:text-2xl fa-file"></i>
                <span class="font-thin text-sm md:text-lg lg:text-2xl">Resume</span>
              </a>
            </p>
          </li>
          <li class="flex-grow-0 inline pr-1 md:pr-2 xl:px-4">
            <p class="text-center">
              <a href="/reading">
                <i class="fas text-green-800 pl-1 md:px-1 text-sm md:text-xl lg:text-2xl fa-book-open"></i>
                <span class="font-thin text-sm md:text-lg lg:text-2xl">Reading</span>
              </a>
            </p>
          </li>
          <li class="flex-grow-0 inline pr-1 md:pr-2 xl:px-4">
            <p class="text-center">
              <a href="/categories.html">
                <i class="fas text-green-800 pl-1 md:px-1 text-sm md:text-xl lg:text-2xl fa-folder-open"></i>
                <span class="font-thin text-sm md:text-lg lg:text-2xl">Categories</span>
              </a>
            </p>
          </li>
          <li class="flex-grow-0 inline pr-1 md:pr-2 xl:px-4">
            <p class="text-center">
              <a href="/posts.html">
                <i class="fas text-green-800 pl-1 md:px-1 text-sm md:text-xl lg:text-2xl fa-archive"></i>
                <span class="font-thin text-sm md:text-lg lg:text-2xl">Archives</span>
              </a>
            </p>
          </li>

        </ul>

      </nav>
    </div>

  </div>
</header>
  <div class="container mx-auto max-w-8xl px-4 flex flex-row flex-grow">
    <div class="w-full md:w-3/4">
      
<main class="main" role="main"><div class="content container mx-auto max-w-6xl">
  <article id="-" class="" itemscope itemtype="http://schema.org/BlogPosting">
    <div class="pt-4">
      <span class="font-semibold text-3xl"><h1 itemprop="name">
  <a class="" href="/papers/formalizing_trust_ai_goldberg.html">[Paper] Formalizing Trust in Artificial Intelligence: Prerequisites, Causes and Goals of Human Trust in AI</a>
</h1>
      </span>
      <div class="pb-4"><i class="fas fa-calendar-check text-gray-500 pr-1"></i>
<time datetime="2021-03-11 00:00:00 &#43;0000 UTC" class="text-sm text-gray-600"
  itemprop="datePublished">2021-03-11</time>
<span class="text-sm text-gray-600" itemprop="wordCount">
	<i class="fas fa-pencil-alt text-gray-500 pl-4 pr-2"></i>1187 words
</span><i class="fas fa-folder-open text-gray-500 pl-4 pr-1"></i>
<a class="text-sm text-gray-600" href=" /categories/papers.html"> papers, </a>
<span class="article-tag">
  <i class="fas fa-tag text-gray-500 pl-4 pr-1"></i>
  <a class="text-sm text-gray-600" href="/tags/reading-club.html"> reading club, </a>
  <a class="text-sm text-gray-600" href="/tags/facct2021.html"> facct2021, </a>
</span><span class="article-category text-sm text-gray-600 pl-4">
  <i class="fa fa-users "></i>
  Alon Jacovi, Ana Marasović, Tim Miller, Yoav Goldberg
</span>
      </div>
    </div>
    <div class="rich-text" itemprop="articleBody">
      <p>What does it mean for a human to trust an AI system? What are the requirements for it to be present, and how do we know if it is? This paper, which just appeared at FAccT, explores these questions.</p>
<p><strong>Authors</strong>: Alon Jacovi, Ana Marasović, Tim Miller, Yoav Goldberg</p>
<p><strong>Link</strong>: <a href="https://arxiv.org/abs/2010.07487">arXiv</a>; from FAccT 2021.</p>
<h2 id="background"><a class="not-rich" href="#background"><i class="fas fa-link"></i></a> Background</h2>
<p>&ldquo;Trust&rdquo; is often a desirable property of interactions between people and AI, and it underlies the desire for explainable AI. But what does trust actually mean? What is a human trusting an AI model with? What are the prerequisites to building this trust?</p>
<p>The abstract summarizes how the authors will explore trust:</p>
<blockquote>
<p>We discuss a model of trust inspired by, but not identical to, sociology&rsquo;s interpersonal trust (i.e., trust between people). This model rests on two key properties of the <em>vulnerability</em> of the user and the ability to <em>anticipate</em> the impact of the AI model&rsquo;s decisions. We incorporate a formalization of &lsquo;contractual trust&rsquo;, such that trust between a user and an AI is trust that some implicit or explicit contract will hold, and a formalization of &lsquo;trustworthiness&rsquo; (which detaches from the notion of trustworthiness in sociology), and with it concepts of &lsquo;warranted&rsquo; and &lsquo;unwarranted&rsquo; trust.</p>
</blockquote>
<p>My post here will mirror the structure of the paper.</p>
<h2 id="defining-trust"><a class="not-rich" href="#defining-trust"><i class="fas fa-link"></i></a> Defining trust</h2>
<p>The authors ground their definition of human-AI trust in sociology. In one formulation, &ldquo;if A believes that B will act in A&rsquo;s best interest, and accepts vulnerability to B&rsquo;s actions, then A trusts B.&rdquo;</p>
<p>Vulnerability is associated with risk. Being vulnerable requires accepting the risk in trusting someone else. (This statement has nothing to do with AI—that&rsquo;s what that means in interpersonal relations.) That applies to AI, too.</p>
<p>What about anticipation, though? In this context, anticipation means &ldquo;believing that the other will act in your best interests.&rdquo; But what specifically does a human anticipate from an AI? That its decisions are correct, or unbiased, or deterministic? The authors frame this as part of a bigger discussion about <em>contracts</em>.</p>
<h3 id="correctness-and-contracts"><a class="not-rich" href="#correctness-and-contracts"><i class="fas fa-link"></i></a> Correctness and contracts</h3>
<p>Model correctness is a general case, they write, of <em>contractual trust</em>. This is the belief that a trustee will stick to a specific contract, e.g., of transparency, reproducibility, or privacy. They include a table from the European Commission on trustworthy AI and use it to provide examples of how to increase contractual trust:</p>
<figure>
    <img src="formalizing_trust_ai_goldberg_fig1.png"/> 
</figure>

<p>&ldquo;An AI model is trustworthy to some contract if it is capable of maintaining this contract,&rdquo; they write. If you trust an AI model, you believe that it will uphold some contracts.</p>
<p>The authors are careful to distinguish <em>trust</em> (an attitude of a person) from &ldquo;trustworthiness&rdquo; (a property of an AI model). One can trust an untrustworthy model, and one can be untrusting of a trustworthy model. They call trust <em>warranted</em> if it&rsquo;s the result of trustworthiness, and <em>unwarranted</em> otherwise. When pursuing trust, we should obviously pursue warranted trust, but we should also <em>minimize unwarranted trust.</em></p>
<h3 id="building-trust-in-ai"><a class="not-rich" href="#building-trust-in-ai"><i class="fas fa-link"></i></a> Building trust in AI</h3>
<p>The authors ask: &ldquo;What causes a model to be trustworthy? And what enables trustworthy models to incur trust?&rdquo;</p>
<p><strong>Intrinsic trust</strong> is built when a model&rsquo;s <em>observable</em> decision process aligns with a user&rsquo;s beliefs. This requires a user to understand the decision process <em>and</em> for them to have prior beliefs. To build intrinsic trust, then, understand who your users are and frame the model&rsquo;s decision-making process in a way that they can understand.</p>
<p><strong>Extrinsic trust</strong> is built through evaluation of results. This requires trusting the evaluation scheme (that it used an appropriate metric, that the training data distribution matches the unseen future data, etc.), and the authors go into detail about how this must happen.</p>
<p>And so the authors extend a common claim in explainable AI (XAI) from:</p>
<blockquote>
<p>XAI for Trust (common): A key motivation of XAI and interpretability is to increase the trust of users in the AI.</p>
</blockquote>
<p>to:</p>
<blockquote>
<p>XAI for Trust (extended): A key motivation of XAI and interpretability is to (1) increase the trustworthiness of the AI, (2) increase the trust of the user in a trustworthy AI, or (3) increase the distrust of the user in a non-trustworthy AI, all corresponding to a stated contract, so that the user develops warranted trust or distrust in that contract.</p>
</blockquote>
<p>This builds on all the concepts presented previously, which makes it a nice summarization of the paper. Make AI more trustworthy, make people trust trustworthy AI more, and make people distrust non-trustworthy AI more. This all occurs with respect to a <em>contract</em> of performance, fairness, privacy, or something else.</p>
<p>The main contribution here is defining previously-fuzzy trust-related terms. What does human-AI trust mean? What is required for it to exist? When is it warranted (and what does that mean?)? The paper answers all of these.</p>
<h2 id="my-thoughts"><a class="not-rich" href="#my-thoughts"><i class="fas fa-link"></i></a> My thoughts</h2>
<p>I find the discussion of warranted vs. unwarranted trust to be interesting. It didn&rsquo;t occur to me that one can design around a lousy AI model to make it appear trustworthy, but in hindsight this is obvious. Providing a sleek interface or explanations can build trust in a model without requiring the model itself to be trustworthy. People dress up bad software every day.</p>
<p>The authors frame this as an ethical issue, which I agree with. &ldquo;In this work, we assume that it is a core motivation of the field to remain ethical and not implement or prioritize unwarranted trust.&rdquo;</p>
<p>Unwarranted trust reminds me of the CHI paper, <a href="/papers/interpreting_interpretability_kaur.html">Interpreting Interpretability</a> by Kaur et al., which showed how data scientists overtrust interpretability tools. Misapplying interpretability tools is a great example of how to (unethically) build <em>unwarranted</em> trust in a model.</p>
<hr>
<p>I liked the discussion of how human-AI trust differs from human-human trust. In human-human trust from sociology, trust is justified if it was not betrayed. (I&rsquo;m sure this is an oversimplification, but I&rsquo;m no sociologist.) For human-AI trust, though, trust is justified if the trustee is trustworthy to some contract.</p>
<p>The human-AI definition is stricter. If a trustee catches a cold and can&rsquo;t fulfill their promise, but in principle could, it&rsquo;s difficult to assign blame. But this doesn&rsquo;t apply to AI. An AI&rsquo;s capabilities are well-defined and it does not have intent. Its trustworthiness can only be evaluated at the face value of its actions.</p>
<p>This reminds me of the <a href="/papers/stochastic_parrots_bender.html">Stochastic Parrots language models paper</a>, which drew a distinction between language that comes from humans (a two-way act of communciation with intent and nuance) and language that comes from language models (which is just text).</p>
<p>The Stochastic Parrots paper provides an example of why trust in AI must be different from trust in humans. If a human says something harmul, they have the opportunity to clarify their intent and restore trust that would have otherwise been broken. A language model that produces harmful text has no intent. It can only be evaluated based on its outputs.</p>
<hr>
<p>I liked this paper overall, but had trouble wrapping my head around some of the more abstract concepts discussed here. I think it will help me to see future work that cites this. I think I agree with their formulation of trust, but don&rsquo;t really know enough to critique it, so I look forward to reading more in the future.</p>

      <p><i class="fas fa-square font-green-800"></i></p>
    </div>
    <div class="font-semibold text-green-800 pt-4">
      <a href="/"><i class="fas fa-arrow-left"></i> Return home</a>
    </div>
  </article>
</div><nav class="bar bar-footer clearfix" data-stick-bottom>
    <div class="bar-inner">
        <ul class="pager pull-left">
            <li class="prev">
                <a href="/papers/we_are_dynamo_salehi.html" title="[Paper] We Are Dynamo: Overcoming Stalling and Friction in Collective Action for Crowd Workers"><i
                        class="icon icon-angle-left"
                        aria-hidden="true"></i><span>&nbsp;&nbsp;</span></a>
            </li>
            <li class="next">
                <a href="/papers/primer_in_bertology_rogers.html"
                    title="[Paper] A Primer in BERTology: What We Know About How BERT Works"><span>&nbsp;&nbsp;</span><i
                        class="icon icon-angle-right" aria-hidden="true"></i></a>
            </li>
        </ul>
    </div>
</nav>

</main>
    </div>
    <div class="hidden w-0 md:block md:w-1/4 md:pt-8 md:pl-12"><aside class="" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="pb-8">
    <h3 class="font-semibold text-xl">Categories</h3>
    <ul class="">
        <li class="">+ <a href="/categories/books.html" class="font-bold text-green-900">books
            </a><span class="">25</span>
        </li>
        <li class="">+ <a href="/categories/general.html" class="font-bold text-green-900">general
            </a><span class="">45</span>
        </li>
        <li class="">+ <a href="/categories/papers.html" class="font-bold text-green-900">papers
            </a><span class="">96</span>
        </li>
        <li class="">+ <a href="/categories/projects.html" class="font-bold text-green-900">projects
            </a><span class="">7</span>
        </li>
        <li class="">+ <a href="/categories/self.html" class="font-bold text-green-900">self
            </a><span class="">5</span>
        </li>
        <li class="">+ <a href="/categories/spark.html" class="font-bold text-green-900">spark
            </a><span class="">34</span>
        </li>
        <li class="">+ <a href="/categories/what-i-read.html" class="font-bold text-green-900">what-i-read
            </a><span class="">53</span>
        </li>
    </ul>
</div>
<div class="pb-4">
    <h3 class="font-semibold text-xl">Recent Posts</h3>
    <ul class="recent-post-list list-unstyled no-thumbnail">
        <li class="pb-4">
            <p>
                <a href="/papers/data_cascades_ai_sambasivan.html" class="font-bold text-green-900">[Paper] &#39;Everyone wants to do the model work, not the data work&#39;: Data Cascades in High-Stakes AI</a>
                <time datetime="2021-04-07 00:00:00 &#43;0000 UTC" itemprop="datePublished" class="text-gray-600 text-sm">2021-04-07
                </time>
            </p>

        </li>
        <li class="pb-4">
            <p>
                <a href="/papers/suicidal_on_sunday_pendse.html" class="font-bold text-green-900">[Paper] &#39;Can I Not Be Suicidal on a Sunday?&#39; Understanding Technology-Mediated Pathways to Mental Health Support</a>
                <time datetime="2021-04-04 00:00:00 &#43;0000 UTC" itemprop="datePublished" class="text-gray-600 text-sm">2021-04-04
                </time>
            </p>

        </li>
        <li class="pb-4">
            <p>
                <a href="/posts/starting_tempus.html" class="font-bold text-green-900">Personal news: starting a new job!</a>
                <time datetime="2021-03-31 00:00:00 &#43;0000 UTC" itemprop="datePublished" class="text-gray-600 text-sm">2021-03-31
                </time>
            </p>

        </li>
        <li class="pb-4">
            <p>
                <a href="/papers/primer_in_bertology_rogers.html" class="font-bold text-green-900">[Paper] A Primer in BERTology: What We Know About How BERT Works</a>
                <time datetime="2021-03-25 00:00:00 &#43;0000 UTC" itemprop="datePublished" class="text-gray-600 text-sm">2021-03-25
                </time>
            </p>

        </li>
        <li class="pb-4">
            <p>
                <a href="/papers/formalizing_trust_ai_goldberg.html" class="font-bold text-green-900">[Paper] Formalizing Trust in Artificial Intelligence: Prerequisites, Causes and Goals of Human Trust in AI</a>
                <time datetime="2021-03-11 00:00:00 &#43;0000 UTC" itemprop="datePublished" class="text-gray-600 text-sm">2021-03-11
                </time>
            </p>

        </li>
    </ul>
</div><div class="pb-12">
    <h3 class="font-semibold text-xl">Tags</h3>
    <ul class="">
        <li class="">+ <a href="/tags/#republic.html" class="font-bold text-green-900">#republic
            </a><span class="">5</span>
        </li>
        <li class="">+ <a href="/tags/chi.html" class="font-bold text-green-900">chi
            </a><span class="">1</span>
        </li>
        <li class="">+ <a href="/tags/chi2019.html" class="font-bold text-green-900">chi2019
            </a><span class="">3</span>
        </li>
        <li class="">+ <a href="/tags/chi2020.html" class="font-bold text-green-900">chi2020
            </a><span class="">20</span>
        </li>
        <li class="">+ <a href="/tags/chi2021.html" class="font-bold text-green-900">chi2021
            </a><span class="">3</span>
        </li>
        <li class="">+ <a href="/tags/cscw2020.html" class="font-bold text-green-900">cscw2020
            </a><span class="">8</span>
        </li>
        <li class="">+ <a href="/tags/facct2021.html" class="font-bold text-green-900">facct2021
            </a><span class="">3</span>
        </li>
        <li class="">+ <a href="/tags/fat2020.html" class="font-bold text-green-900">fat2020
            </a><span class="">2</span>
        </li>
        <li class="">+ <a href="/tags/iclr.html" class="font-bold text-green-900">iclr
            </a><span class="">2</span>
        </li>
        <li class="">+ <a href="/tags/icwsm2020.html" class="font-bold text-green-900">icwsm2020
            </a><span class="">5</span>
        </li>
        <li class="">+ <a href="/tags/indistractable.html" class="font-bold text-green-900">indistractable
            </a><span class="">2</span>
        </li>
        <li class="">+ <a href="/tags/neurips.html" class="font-bold text-green-900">neurips
            </a><span class="">2</span>
        </li>
        <li class="">+ <a href="/tags/password-tool.html" class="font-bold text-green-900">password-tool
            </a><span class="">6</span>
        </li>
        <li class="">+ <a href="/tags/reading-club.html" class="font-bold text-green-900">reading-club
            </a><span class="">28</span>
        </li>
        <li class="">+ <a href="/tags/recsys.html" class="font-bold text-green-900">recsys
            </a><span class="">1</span>
        </li>
        <li class="">+ <a href="/tags/site.html" class="font-bold text-green-900">site
            </a><span class="">6</span>
        </li>
        <li class="">+ <a href="/tags/www.html" class="font-bold text-green-900">www
            </a><span class="">3</span>
        </li>
    </ul>
</div>
</aside><footer class="" itemscope itemtype="http://schema.org/WPFooter">
  <div class="py-4 text-sm">
    &copy; 2019 - 2020 Tushar Chandra
    <p style="margin-bottom: 4px">All opinions are my own.</p>
    <div class="publishby">Powered by Hugo with a custom theme.</div>
  </div>
  <ul class="">
    <li class="inline text-xl pr-4">
      <a href="https://github.com/tuchandra" target="_blank" title="github">
        <i class="fab fa-github"></i>
      </a>
    </li>
    <li class="inline text-xl pr-4">
      <a href="https://www.linkedin.com/in/tushar-chandra-76a623b6/" target="_blank" title="linkedin">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
    <li class="inline text-xl pr-4">
      <a href="mailto:me@tusharc.dev" target="_blank" title="envelope">
        <i class="fas fa-envelope"></i>
      </a>
    </li>
    <li class="inline text-xl pr-4">
      <a href="/index.xml" target="_blank" title="rss">
        <i class="fas fa-rss"></i>
      </a>
    </li>

  </ul>
</footer>
    </div>
  </div>
  <div class="mx-auto mt-auto w-full px-4 bg-green-500 mt-4 md:py-8">
    <div class="md:invisible md:hidden max-w-8xl"><footer class="" itemscope itemtype="http://schema.org/WPFooter">
  <div class="py-4 text-sm">
    &copy; 2019 - 2020 Tushar Chandra
    <p style="margin-bottom: 4px">All opinions are my own.</p>
    <div class="publishby">Powered by Hugo with a custom theme.</div>
  </div>
  <ul class="">
    <li class="inline text-xl pr-4">
      <a href="https://github.com/tuchandra" target="_blank" title="github">
        <i class="fab fa-github"></i>
      </a>
    </li>
    <li class="inline text-xl pr-4">
      <a href="https://www.linkedin.com/in/tushar-chandra-76a623b6/" target="_blank" title="linkedin">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
    <li class="inline text-xl pr-4">
      <a href="mailto:me@tusharc.dev" target="_blank" title="envelope">
        <i class="fas fa-envelope"></i>
      </a>
    </li>
    <li class="inline text-xl pr-4">
      <a href="/index.xml" target="_blank" title="rss">
        <i class="fas fa-rss"></i>
      </a>
    </li>

  </ul>
</footer>
    </div>
  </div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_SVG"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
            showMathMenu: false, //disables context menu
            tex2jax: {
            inlineMath: [ ['$','$'], ['\\(','\\)'] ]
           }
    });
</script>







<script data-goatcounter="https://tusharc.goatcounter.com/count" async src="//gc.zgo.at/count.js">
</script>
</body>

</html>