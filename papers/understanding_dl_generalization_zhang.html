<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <title>
    [Paper] Understanding deep learning requires rethinking generalization - Tushar Chandra
    </title>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport"
    content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="renderer" content="webkit">
  <meta http-equiv="Cache-Control" content="no-transform" />
  <meta http-equiv="Cache-Control" content="no-siteapp" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="format-detection" content="telephone=no,email=no,adress=no">
  
  <meta name="theme-color" content="#000000" />
  <meta http-equiv="window-target" content="_top" /><meta name="description" content="This Google Brain paper from ICLR 2017 tackles the question of generalization in neural networks. What causes a network that performs well on training data to also perform well on testing data? (Answer: ¯\_(ツ)_/¯)
" />
  <meta name="generator" content="Hugo 0.70.0" />
  <title>[Paper] Understanding deep learning requires rethinking generalization - Tushar Chandra</title>

  
  
  <link rel="stylesheet" href="/css/styles.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.9.0/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/9.15.10/styles/github.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.css">

  <meta property="og:title" content="[Paper] Understanding deep learning requires rethinking generalization" />
<meta property="og:description" content="This Google Brain paper from ICLR 2017 tackles the question of generalization in neural networks. What causes a network that performs well on training data to also perform well on testing data? (Answer: ¯\_(ツ)_/¯)" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/papers/understanding_dl_generalization_zhang.html" />
<meta property="article:published_time" content="2020-08-12T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-08-12T00:00:00+00:00" />
<meta itemprop="name" content="[Paper] Understanding deep learning requires rethinking generalization">
<meta itemprop="description" content="This Google Brain paper from ICLR 2017 tackles the question of generalization in neural networks. What causes a network that performs well on training data to also perform well on testing data? (Answer: ¯\_(ツ)_/¯)">
<meta itemprop="datePublished" content="2020-08-12T00:00:00&#43;00:00" />
<meta itemprop="dateModified" content="2020-08-12T00:00:00&#43;00:00" />
<meta itemprop="wordCount" content="948">



<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="[Paper] Understanding deep learning requires rethinking generalization"/>
<meta name="twitter:description" content="This Google Brain paper from ICLR 2017 tackles the question of generalization in neural networks. What causes a network that performs well on training data to also perform well on testing data? (Answer: ¯\_(ツ)_/¯)"/>
</head>
</head>



<body class="h-screen flex flex-col justify-between" itemscope itemtype="http://schema.org/WebPage"><header class="w-screen bg-green-500" itemscope itemtype="http://schema.org/WPHeader">
  <div class="flex max-w-8xl container mx-auto">
    <div class="hidden sm:inline flex items-center">
      <a href="/">
        <img class="rounded-full m-1 md:m-4" src="/headshot.jpg" width="100" height="100">
      </a>
    </div>
    <div class="w-full px-4 flex flex-col lg:flex-row justify-between lg:items-center my-4">
      <div class="xl:flex-grow">
        <a href="/">
          <span class="text-lg sm:text-2xl md:text-3xl font-semibold">Tushar Chandra</span>
          <span class="hidden pl-4 lg:inline"><br></span>
          <span class="text-sm md:text-xl font-thin pl-4 md:pl-0 lg:text-xl">Data Scientist / Chicago, IL
          </span>
        </a>
      </div>
      <nav class="" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation">
        <ul class="flex lg:justify-between">
          <li class="flex-grow-0 inline pr-1 md:pr-2 xl:px-4">
            <p class="text-center">
              <a href="/about">
                <i class="fas text-green-800 pl-1 md:px-1 text-sm md:text-xl lg:text-2xl fa-mug-hot"></i>
                <span class="font-thin text-sm md:text-lg lg:text-2xl">About</span>
              </a>
            </p>
          </li>
          <li class="flex-grow-0 inline pr-1 md:pr-2 xl:px-4">
            <p class="text-center">
              <a href="/resume">
                <i class="fas text-green-800 pl-1 md:px-1 text-sm md:text-xl lg:text-2xl fa-file"></i>
                <span class="font-thin text-sm md:text-lg lg:text-2xl">Resume</span>
              </a>
            </p>
          </li>
          <li class="flex-grow-0 inline pr-1 md:pr-2 xl:px-4">
            <p class="text-center">
              <a href="/reading">
                <i class="fas text-green-800 pl-1 md:px-1 text-sm md:text-xl lg:text-2xl fa-book-open"></i>
                <span class="font-thin text-sm md:text-lg lg:text-2xl">Reading</span>
              </a>
            </p>
          </li>
          <li class="flex-grow-0 inline pr-1 md:pr-2 xl:px-4">
            <p class="text-center">
              <a href="/categories.html">
                <i class="fas text-green-800 pl-1 md:px-1 text-sm md:text-xl lg:text-2xl fa-folder-open"></i>
                <span class="font-thin text-sm md:text-lg lg:text-2xl">Categories</span>
              </a>
            </p>
          </li>
          <li class="flex-grow-0 inline pr-1 md:pr-2 xl:px-4">
            <p class="text-center">
              <a href="/posts.html">
                <i class="fas text-green-800 pl-1 md:px-1 text-sm md:text-xl lg:text-2xl fa-archive"></i>
                <span class="font-thin text-sm md:text-lg lg:text-2xl">Archives</span>
              </a>
            </p>
          </li>

        </ul>

      </nav>
    </div>

  </div>
</header>
  <div class="container mx-auto max-w-8xl px-4 flex flex-row flex-grow">
    <div class="w-full md:w-3/4">
      
<main class="main" role="main"><div class="content container mx-auto max-w-6xl">
  <article id="-" class="" itemscope itemtype="http://schema.org/BlogPosting">
    <div class="pt-4">
      <span class="font-semibold text-3xl"><h1 itemprop="name">
  <a class="" href="/papers/understanding_dl_generalization_zhang.html">[Paper] Understanding deep learning requires rethinking generalization</a>
</h1>
      </span>
      <div class="pb-4"><i class="fas fa-calendar-check text-gray-500 pr-1"></i>
<time datetime="2020-08-12 00:00:00 &#43;0000 UTC" class="text-sm text-gray-600"
  itemprop="datePublished">2020-08-12</time>
<span class="text-sm text-gray-600" itemprop="wordCount">
	<i class="fas fa-pencil-alt text-gray-500 pl-4 pr-2"></i>948 words
</span><i class="fas fa-folder-open text-gray-500 pl-4 pr-1"></i>
<a class="text-sm text-gray-600" href=" /categories/papers.html"> papers, </a><span class="article-category text-sm text-gray-600 pl-4">
  <i class="fa fa-users "></i>
  Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, Oriol Vinyals
</span>
      </div>
    </div>
    <div class="rich-text" itemprop="articleBody">
      <p>This Google Brain paper from ICLR 2017 tackles the question of <em>generalization</em> in neural networks. What causes a network that performs well on training data to also perform well on testing data? (Answer: ¯\_(ツ)_/¯)</p>
<p><strong>Authors</strong>: Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, Oriol Vinyals</p>
<p><strong>Link</strong>: <a href="https://arxiv.org/abs/1611.03530">arXiv</a></p>
<h2 id="central-results"><a class="not-rich" href="#central-results"><i class="fas fa-link"></i></a> Central results</h2>
<p>The abstract, broken down:</p>
<ul>
<li>conventional deep learning wisdom &ldquo;attributes small generalization error either to properties of the model family, or to the regularization techniques used during training.&rdquo;</li>
<li>they do &ldquo;extensive systematic experiments&rdquo;</li>
<li>and find that the above explanation does not hold in practice</li>
<li>in particular, a CNN can &ldquo;easily fit a random labeling of the training data&rdquo;</li>
<li>and that happens even if the images are replaced by noise.</li>
</ul>
<p>The central finding of this paper is that &ldquo;deep neural networks easily fit random labels,&rdquo; contradicting conventional wisdom about model architectures, model size, hyperparameters, or optimization.</p>
<p>They went beyond random labels to random <em>noise</em>. CNNs could fit images that were entirely Gaussian noise with zero training error. And when varying the amount of noise (interpolating between no noise and all noise), they observed a &ldquo;steady deterioration of the generalization error as we increase the noise level.&rdquo;</p>
<p>This implies:</p>
<ul>
<li>model architecture is not enough to explain generalization</li>
<li>explicit regularization is not enough to explain generalization</li>
<li>theoretically, a two-layer ReLU network with <em>2n + d</em> parameters can express any labeling of <em>n</em> samples in <em>d</em> dimensions</li>
<li>stochastic gradient descent acts as an implicit regularizer</li>
</ul>
<p>Wow&mdash;crazy stuff.</p>
<h2 id="experiments"><a class="not-rich" href="#experiments"><i class="fas fa-link"></i></a> Experiments</h2>
<p>They run a bunch of experiments on CIFAR10 and ImageNet, standard image classification datasets:</p>
<ul>
<li>true labels</li>
<li>partially corrupted labels (with probability <em>p</em>, the label is replaced by another one)</li>
<li>random labels (they&rsquo;re all random)</li>
<li>shuffled pixels (the pixels in every image are permuted in the same way)</li>
<li>random pixels (the pixels in every image are permuted in a different way)</li>
<li>Gaussian (pixels are generated according randomly to a Gaussian distribution)</li>
</ul>
<p>The networks they test&mdash;Inception v3, AlexNet, and a couple garden variety MLPs&mdash;can fit both the original and random labels with 100% training accuracy. An abridged version of Table 1 in the paper:</p>
<table>
<thead>
<tr>
<th>model</th>
<th>dataset</th>
<th>train accuracy</th>
<th>test accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>Inception</td>
<td>CIFAR10</td>
<td>100.0</td>
<td>89.05</td>
</tr>
<tr>
<td>Inception</td>
<td>CIFAR10, random labels</td>
<td>100.0</td>
<td>9.78</td>
</tr>
<tr>
<td>AlexNet</td>
<td>CIFAR10</td>
<td>99.90</td>
<td>81.22</td>
</tr>
<tr>
<td>AlexNet</td>
<td>CIFAR10, random labels</td>
<td>99.82</td>
<td>9.86</td>
</tr>
<tr>
<td>MLP 3x512</td>
<td>CIFAR10</td>
<td>100.0</td>
<td>53.35</td>
</tr>
<tr>
<td>MLP 3x512</td>
<td>CIFAR10, random labels</td>
<td>100.0</td>
<td>10.48</td>
</tr>
<tr>
<td>MLP 1x512</td>
<td>CIFAR10</td>
<td>99.80</td>
<td>50.39</td>
</tr>
<tr>
<td>MLP 1x512</td>
<td>CIFAR10, random labels</td>
<td>99.34</td>
<td>10.61</td>
</tr>
</tbody>
</table>
<p><em>Abridged Table 1: the training and test accuracy of various models on CIFAR10</em></p>
<p>Some of these results also show up in Figure 1. This Figure also demonstrates that networks can always fit corrupted training sets perfectly, but the test (generalization) errors approach 90%, equivalent to random guessing.</p>
<figure>
    <img src="understanding_dl_generalization_zhang_img1.png"/> 
</figure>

<blockquote>
<p>This leads to a range of intermediate learning problems where there remains some level of signal in the labels.  We observe a steady deterioration of the generalization error as we increase the noise level.   This shows that neural networks are able to apture the remaining signal in the data, while at the same time fit the noisy part using brute-force.</p>
</blockquote>
<h2 id="on-regularization"><a class="not-rich" href="#on-regularization"><i class="fas fa-link"></i></a> On regularization</h2>
<p>The idea behind regularization, both in theory and in practice, is to help mitigate overfitting. You &ldquo;confine learning&rdquo; to a lower-complexity region of training space. Regularization techniques are their own subfield of machine learning research.</p>
<p>This paper studies:</p>
<ul>
<li>data augmentation: adding transformations (random cropping, random perturbations of hue or saturation, etc.) to the training set</li>
<li>weight decay (or L2 regularization)</li>
<li>dropout (mask each weight randomly to zero with some probability)</li>
</ul>
<p>Their basic conclusion (though I didn&rsquo;t spend as much time understanding this) is that explicit regularization helps improve generalization, but is not necessary for it.</p>
<p><em>Implicit</em> regularization (early stopping, batch normalization) similarly helps, showing that &ldquo;early stopping could <em>potentially</em> improve the generalization performance&rdquo; and &ldquo;the [batch] normalization operator helps stabilize the learning dynamics.&rdquo;</p>
<blockquote>
<p>In summary, our observations on both explicit and implicit regularizers are consistently suggesting that regularizers, when properly tuned, could help to improve the generalization performance. However, it is unlikely that the regularizers are the fundamental reason for generalization, as the networks continue to perform well after all the regularizers removed.</p>
</blockquote>
<p>No easy answers here!</p>
<h2 id="finite-sample-expressivity"><a class="not-rich" href="#finite-sample-expressivity"><i class="fas fa-link"></i></a> Finite sample expressivity</h2>
<p>Research on the &ldquo;expressivity&rdquo; of neural networks studies what classes functions (of an entire domain‚ can(not) be represented by certain classes of networks. The authors study expressivity of <em>samples</em>.</p>
<p>The key result is that: for <em>n</em> samples in <em>d</em> dimensions, there exists a two-layer ReLU neural network with <em>2n + d</em> weights that can represent any function on those samples. If you&rsquo;d rather the network be wide than deep, you can have width <em>O(n/k)</em> and depth <em>k</em>. I didn&rsquo;t (try to) understand the proof.</p>
<h2 id="thoughts"><a class="not-rich" href="#thoughts"><i class="fas fa-link"></i></a> Thoughts</h2>
<p>I&rsquo;m not very familiar with the pure machine learning world that this paper sits in. But the results don&rsquo;t seem <em>that</em> surprising to me, which makes me feel like I&rsquo;m missing something. I thought it was known that neural nets could express any function, and so the fact that they can memorize random labels is <em>bad</em> but not <em>surprising.</em></p>
<p>I think this paper was exceptionally well-written. It appears to have been impactful (it&rsquo;s been cited ~2000 times in 3 years) while also being easy to read. While I skipped the proofs in the appendix, the main results were clearly presented and the structure of the paper (with a lengthy introduction) was helpful.</p>
<p>Where does this leave us, in terms of &ldquo;rethinking generalization&rdquo;? The authors claim that it means existing statistical learning theory is inadequate why neural nets do and do not generalize. I am not sure I understand this point.</p>
<p>Some resources I used while reading this were <a href="https://blog.acolyer.org/2017/05/11/understanding-deep-learning-requires-re-thinking-generalization/">The Morning Paper</a> and <a href="https://danieltakeshi.github.io/2017/05/19/understanding-deep-learning-requires-rethinking-generalization-my-thoughts-and-notes">Daniel Seita&rsquo;s blog</a>.</p>

      <p><i class="fas fa-square font-green-800"></i></p>
    </div>
    <div class="font-semibold text-green-800 pt-4">
      <a href="/"><i class="fas fa-arrow-left"></i> Return home</a>
    </div>
  </article>
</div><nav class="bar bar-footer clearfix" data-stick-bottom>
    <div class="bar-inner">
        <ul class="pager pull-left">
            <li class="prev">
                <a href="/what_i_read/20200808.html" title="What I read this week (August 2 - 8)"><i
                        class="icon icon-angle-left"
                        aria-hidden="true"></i><span>&nbsp;&nbsp;</span></a>
            </li>
            <li class="next">
                <a href="/papers/modeling_political_attention_hemphill.html"
                    title="[Paper] Two Computational Models for Analyzing Political Attention in Social Media"><span>&nbsp;&nbsp;</span><i
                        class="icon icon-angle-right" aria-hidden="true"></i></a>
            </li>
        </ul>
    </div>
</nav>

</main>
    </div>
    <div class="hidden w-0 md:block md:w-1/4 md:pt-8 md:pl-12"><aside class="" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="pb-8">
    <h3 class="font-semibold text-xl">Categories</h3>
    <ul class="">
        <li class="">+ <a href="/categories/books.html" class="font-bold text-green-900">books
            </a><span class="">12</span>
        </li>
        <li class="">+ <a href="/categories/general.html" class="font-bold text-green-900">general
            </a><span class="">36</span>
        </li>
        <li class="">+ <a href="/categories/papers.html" class="font-bold text-green-900">papers
            </a><span class="">58</span>
        </li>
        <li class="">+ <a href="/categories/projects.html" class="font-bold text-green-900">projects
            </a><span class="">7</span>
        </li>
        <li class="">+ <a href="/categories/self.html" class="font-bold text-green-900">self
            </a><span class="">3</span>
        </li>
        <li class="">+ <a href="/categories/spark.html" class="font-bold text-green-900">spark
            </a><span class="">34</span>
        </li>
        <li class="">+ <a href="/categories/what-i-read.html" class="font-bold text-green-900">what-i-read
            </a><span class="">37</span>
        </li>
    </ul>
</div>
<div class="pb-4">
    <h3 class="font-semibold text-xl">Recent Posts</h3>
    <ul class="recent-post-list list-unstyled no-thumbnail">
        <li class="pb-4">
            <p>
                <a href="/what_i_read/20200822.html" class="font-bold text-green-900">What I read this week (August 16 - 22)</a>
                <time datetime="2020-08-22 00:00:00 &#43;0000 UTC" itemprop="datePublished" class="text-gray-600 text-sm">2020-08-22
                </time>
            </p>

        </li>
        <li class="pb-4">
            <p>
                <a href="/papers/content_moderation_scale_gillespie.html" class="font-bold text-green-900">[Paper] Content moderation, AI, and the question of scale</a>
                <time datetime="2020-08-21 15:00:00 &#43;0000 UTC" itemprop="datePublished" class="text-gray-600 text-sm">2020-08-21
                </time>
            </p>

        </li>
        <li class="pb-4">
            <p>
                <a href="/papers/media_coverage_presidential_google_kawakami.html" class="font-bold text-green-900">[Paper] The Media Coverage of the 2020 US Presidential Election Candidates through the Lens of Google&#39;s Top Stories</a>
                <time datetime="2020-08-21 00:00:00 &#43;0000 UTC" itemprop="datePublished" class="text-gray-600 text-sm">2020-08-21
                </time>
            </p>

        </li>
        <li class="pb-4">
            <p>
                <a href="/posts/site_changes_tags.html" class="font-bold text-green-900">Minor website addition: tags and anchors</a>
                <time datetime="2020-08-21 00:00:00 &#43;0000 UTC" itemprop="datePublished" class="text-gray-600 text-sm">2020-08-21
                </time>
            </p>

        </li>
        <li class="pb-4">
            <p>
                <a href="/papers/auditing_news_curation_systems_diakopoulos.html" class="font-bold text-green-900">[Paper] Auditing News Curation Systems: A Case Study Examining Algorithmic and Editorial Logic in Apple News</a>
                <time datetime="2020-08-20 00:00:00 &#43;0000 UTC" itemprop="datePublished" class="text-gray-600 text-sm">2020-08-20
                </time>
            </p>

        </li>
    </ul>
</div><div class="pb-12">
    <h3 class="font-semibold text-xl">Tags</h3>
    <ul class="">
        <li class="">+ <a href="/tags/chi2020.html" class="font-bold text-green-900">chi2020
            </a><span class="">20</span>
        </li>
        <li class="">+ <a href="/tags/fat2020.html" class="font-bold text-green-900">fat2020
            </a><span class="">2</span>
        </li>
        <li class="">+ <a href="/tags/icwsm2020.html" class="font-bold text-green-900">icwsm2020
            </a><span class="">5</span>
        </li>
        <li class="">+ <a href="/tags/password-tool.html" class="font-bold text-green-900">password-tool
            </a><span class="">6</span>
        </li>
        <li class="">+ <a href="/tags/site.html" class="font-bold text-green-900">site
            </a><span class="">5</span>
        </li>
    </ul>
</div>
</aside><footer class="" itemscope itemtype="http://schema.org/WPFooter">
  <div class="py-4 text-sm">
    &copy; 2019 - 2020 Tushar Chandra
    <p style="margin-bottom: 4px">All opinions are my own.</p>
    <div class="publishby">Powered by Hugo with a custom theme.</div>
  </div>
  <ul class="">
    <li class="inline text-xl pr-4">
      <a href="https://github.com/tuchandra" target="_blank" title="github">
        <i class="fab fa-github"></i>
      </a>
    </li>
    <li class="inline text-xl pr-4">
      <a href="https://www.linkedin.com/in/tushar-chandra-76a623b6/" target="_blank" title="linkedin">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
    <li class="inline text-xl pr-4">
      <a href="mailto:me@tusharc.dev" target="_blank" title="envelope">
        <i class="fas fa-envelope"></i>
      </a>
    </li>
    <li class="inline text-xl pr-4">
      <a href="/index.xml" target="_blank" title="rss">
        <i class="fas fa-rss"></i>
      </a>
    </li>

  </ul>
</footer>
    </div>
  </div>
  <div class="mx-auto w-screen px-4 bg-green-500 mt-4 md:py-8">
    <div class="md:invisible md:hidden max-w-8xl"><footer class="" itemscope itemtype="http://schema.org/WPFooter">
  <div class="py-4 text-sm">
    &copy; 2019 - 2020 Tushar Chandra
    <p style="margin-bottom: 4px">All opinions are my own.</p>
    <div class="publishby">Powered by Hugo with a custom theme.</div>
  </div>
  <ul class="">
    <li class="inline text-xl pr-4">
      <a href="https://github.com/tuchandra" target="_blank" title="github">
        <i class="fab fa-github"></i>
      </a>
    </li>
    <li class="inline text-xl pr-4">
      <a href="https://www.linkedin.com/in/tushar-chandra-76a623b6/" target="_blank" title="linkedin">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
    <li class="inline text-xl pr-4">
      <a href="mailto:me@tusharc.dev" target="_blank" title="envelope">
        <i class="fas fa-envelope"></i>
      </a>
    </li>
    <li class="inline text-xl pr-4">
      <a href="/index.xml" target="_blank" title="rss">
        <i class="fas fa-rss"></i>
      </a>
    </li>

  </ul>
</footer>
    </div>
  </div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_SVG"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
            showMathMenu: false, //disables context menu
            tex2jax: {
            inlineMath: [ ['$','$'], ['\\(','\\)'] ]
           }
    });
</script>


<script type="text/javascript" src="https://cdn.staticfile.org/highlight.js/9.15.10/highlight.min.js"></script>
<script type="text/javascript" src="https://cdn.staticfile.org/highlight.js/9.15.10/languages/python.min.js"></script><script>
    hljs.configure({
        tabReplace: '    ', 
        classPrefix: ''     
        
    })
    hljs.initHighlightingOnLoad();
</script>


<script data-goatcounter="https://tusharc.goatcounter.com/count" async src="//gc.zgo.at/count.js">
</script>
</body>

</html>