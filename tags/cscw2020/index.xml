<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>cscw2020 on Tushar Chandra</title>
    <link>/tags/cscw2020.html</link>
    <description>Recent content in cscw2020 on Tushar Chandra</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 18 Nov 2020 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/tags/cscw2020/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>[Paper] Metaphors in Moderation</title>
      <link>/papers/metaphors_in_moderation_seering.html</link>
      <pubDate>Fri, 09 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/papers/metaphors_in_moderation_seering.html</guid>
      <description>&lt;p&gt;How do moderators of online communities see their roles? As a firefighter, as police, as a gardener, as a mediator? This paper explores the metaphors that volunteer moderators use to make sense of their roles.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>[Paper] &#39;Everyone wants to do the model work, not the data work&#39;: Data Cascades in High-Stakes AI</title>
      <link>/papers/data_cascades_ai_sambasivan.html</link>
      <pubDate>Wed, 07 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/papers/data_cascades_ai_sambasivan.html</guid>
      <description>&lt;p&gt;Data work is foundational to any AI application, but often undervalued. In practice, this results in &amp;ldquo;data cascades,&amp;rdquo; which occur when data issues compound into downstream technical debt. This work studies how data cascades happen, what they cause, and how to avoid them.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>[Paper] &#39;Can I Not Be Suicidal on a Sunday?&#39; Understanding Technology-Mediated Pathways to Mental Health Support</title>
      <link>/papers/suicidal_on_sunday_pendse.html</link>
      <pubDate>Sun, 04 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/papers/suicidal_on_sunday_pendse.html</guid>
      <description>&lt;p&gt;How do the designs of mental health helplines influence people&amp;rsquo;s experiences with them? This paper weaves together the lived experience of 18 helpline stakeholders to understand barriers along the pathway to care.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Personal news: starting a new job!</title>
      <link>/posts/starting_tempus.html</link>
      <pubDate>Wed, 31 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>/posts/starting_tempus.html</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve decided to move on from both Nielsen and data science and start a new role in engineering. This is a short post with some reflection.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>[Paper] A Primer in BERTology: What We Know About How BERT Works</title>
      <link>/papers/primer_in_bertology_rogers.html</link>
      <pubDate>Thu, 25 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>/papers/primer_in_bertology_rogers.html</guid>
      <description>&lt;p&gt;BERT is one of many large language models taking NLP by storm. But little is known about how or why it works, leading to papers studying it specifically: &amp;ldquo;BERTology.&amp;rdquo; This survey paper synthesizes current research on how BERT works and what remains unknown.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>[Paper] Formalizing Trust in Artificial Intelligence: Prerequisites, Causes and Goals of Human Trust in AI</title>
      <link>/papers/formalizing_trust_ai_goldberg.html</link>
      <pubDate>Thu, 11 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>/papers/formalizing_trust_ai_goldberg.html</guid>
      <description>&lt;p&gt;What does it mean for a human to trust an AI system? What are the requirements for it to be present, and how do we know if it is? This paper, which just appeared at FAccT, explores these questions.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>[Paper] We Are Dynamo: Overcoming Stalling and Friction in Collective Action for Crowd Workers</title>
      <link>/papers/we_are_dynamo_salehi.html</link>
      <pubDate>Thu, 04 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>/papers/we_are_dynamo_salehi.html</guid>
      <description>&lt;p&gt;What does collective action look like online? The authors design and study Dynamo, a platform for organizing Amazon Mechanical Turk workers into action, and discuss the unique affordances and challenges of the web.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>[Paper] Datasheets for Datasets</title>
      <link>/papers/datasheets_for_datasets.html</link>
      <pubDate>Thu, 25 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/papers/datasheets_for_datasets.html</guid>
      <description>&lt;p&gt;Datasets, which are a central part of all machine learning, are often alarmingly undocumented. How can we do this better, prioritizing transparency and accountability? The authors propose datasheets that describe the characteristics, collection practices, and recommended uses of datasets.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>[Paper] On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ðŸ¦œ</title>
      <link>/papers/stochastic_parrots_bender.html</link>
      <pubDate>Thu, 11 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/papers/stochastic_parrots_bender.html</guid>
      <description>&lt;p&gt;Large language models are increasing in prevalence, and the field is marching steadily towards bigger ones still. This paper takes a step back to question the risks of these models. Can they be too big? (And yesâ€”the emoji is part of the title!)&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Recurring Reading</title>
      <link>/self/recurring_reading.html</link>
      <pubDate>Fri, 05 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/self/recurring_reading.html</guid>
      <description>&lt;p&gt;I note all of the reading that I do on a semi-regular basis. I compiled this partially as a reference for myself, and partially to point others to.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>[Paper] Explaining Explainability: Towards Social Transparency in AI Systems</title>
      <link>/papers/explaining_explainability_ehsan.html</link>
      <pubDate>Thu, 28 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/papers/explaining_explainability_ehsan.html</guid>
      <description>&lt;p&gt;Explainable AI is often treated as an algorithmic problem, but this framing leaves a blind spot of how an AI system fits into an actual organization. This paper uses the idea of social transparency to motivate a new, more practical framework for thinking about explainability.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>[Paper] From Optimizing Engagement to Measuring Value</title>
      <link>/papers/from_engagement_to_value_hardt.html</link>
      <pubDate>Thu, 21 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/papers/from_engagement_to_value_hardt.html</guid>
      <description>&lt;p&gt;Measuring &amp;lsquo;engagement&amp;rsquo; on social platforms is always going to be a proxy for an actual concept of value; a user engaging with something doesn&amp;rsquo;t mean they value it. This paper closes that gap, connecting engagement behaviors to value through a Bayesian network. The authors implement their approach on Twitter.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>More thoughts on &#39;Street-Level Algorithms&#39;</title>
      <link>/papers/street_level_algorithms_alkhatib_2.html</link>
      <pubDate>Fri, 15 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/papers/street_level_algorithms_alkhatib_2.html</guid>
      <description>&lt;p&gt;Talking to Judah in our reading club helped me to crystallize some of my thoughts about Ali Alkhatib &amp;amp; Michael Bernstein&amp;rsquo;s &lt;em&gt;Street-Level Algorithms&lt;/em&gt; paper. This post explores these.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>[Paper] Street-Level Algorithms: A Theory at the Gaps between Policy and Decisions</title>
      <link>/papers/street_level_algorithms_alkhatib.html</link>
      <pubDate>Thu, 14 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/papers/street_level_algorithms_alkhatib.html</guid>
      <description>&lt;p&gt;Street-level bureaucrats are the people making routine decisions for institutionsâ€”administrators, police, professors, and more. This work introduces &lt;em&gt;street-level algorithms&lt;/em&gt; as an idea for algorithms that are tasked with filling the same role.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>What I read this week (January 3 - 9)</title>
      <link>/what_i_read/20210109.html</link>
      <pubDate>Sat, 09 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/what_i_read/20210109.html</guid>
      <description>&lt;p&gt;Two thoughtful articles this week: on how machine learning is going real time, and the problems with machine learning in medicine.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>[Paper] On the Legal Compatibility of Fairness Definitions</title>
      <link>/papers/legal_compatibility_fairness_definitions_xiang.html</link>
      <pubDate>Thu, 07 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/papers/legal_compatibility_fairness_definitions_xiang.html</guid>
      <description>&lt;p&gt;Fairness in machine learning is typically concerned with ideas like discrimination, disparate impact or treatment, or protected classes. This paper describes how the definitions being used in ML aren&amp;rsquo;t always compatible with definitions in the legal system.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>My 2020 in Review</title>
      <link>/posts/year_in_review_2020.html</link>
      <pubDate>Tue, 05 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/posts/year_in_review_2020.html</guid>
      <description>&lt;p&gt;My year in review post. 2020 was a terrible year, all things considered, but I found opportunity to rekindle my reading habit and write consistently. Inside are reflections on personal and career growth, the development of my research skills, and some of the best things I read this year.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>More minor site changes: cleaning up category pages</title>
      <link>/posts/cleanup_category_pages.html</link>
      <pubDate>Mon, 04 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/posts/cleanup_category_pages.html</guid>
      <description>&lt;p&gt;I did some cleanup of the categories and tag listings. This short post discusses why and how!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Resume</title>
      <link>/resume/</link>
      <pubDate>Mon, 04 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/resume/</guid>
      <description>&lt;style type=&#34;text/css&#34;&gt;.rich-text li {line-height: 1.5;}&lt;/style&gt;
&lt;p&gt;I&amp;rsquo;m currently a senior data scientist at &lt;a href=&#34;https://www.nielsen.com/us/en/solutions/nielsen-global-media/&#34;&gt;Nielsen&lt;/a&gt;. I did my master&amp;rsquo;s in computer science and undergrad in computer science and the &lt;a href=&#34;httpqs://www.isp.northwestern.edu/&#34;&gt;Integrated Science Program&lt;/a&gt;, both at Northwestern (go &amp;lsquo;cats!). My email address is me@(this domain).&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>What I read this week (December 27 - January 2)</title>
      <link>/what_i_read/20210102.html</link>
      <pubDate>Sat, 02 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/what_i_read/20210102.html</guid>
      <description>&lt;p&gt;Happy new year! This week&amp;rsquo;s edition is short, featuring a post on Apple&amp;rsquo;s new M1 chips and why they&amp;rsquo;re so fast, and another on boundaries within engineering orgs, especially &amp;ldquo;social good&amp;rdquo; ones.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
